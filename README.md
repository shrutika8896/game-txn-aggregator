# ğŸ•¹ï¸ Game Transaction Aggregator

This microservice collects game transaction data from a mocked API, aggregates per-user balances, and exposes useful endpoints.

## Features
- ğŸ§  MongoDB persistence
- â±ï¸ Cron (every 12s) to fetch transaction data
- ğŸ§¾ Aggregates: earned, spent, payout, paidOut, balance
- ğŸ”„ Resumable & scalable

## Requirements
- Node.js 18+
- Docker

## Getting Started
```bash
# Clone the repo
https://github.com/<your-username>/game-txn-aggregator.git
cd game-txn-aggregator

# Start MongoDB
docker-compose up -d

# Install deps
npm install

# Run the app
npm run start:dev
```

## API Endpoints
```http
GET /user/:userId/summary     â†’ user aggregation
GET /payouts                  â†’ list of requested payouts
```
curl requests:
```
curl --location --request GET 'http://localhost:3000/user/074093/summary' \
--header 'Content-Type: application/json' \
--data '{
    "username": "shrutika"
}'

curl --location --request GET 'http://localhost:3000/payouts' \
--header 'Content-Type: application/json' \
--data '{
    "username": "shrutika"
}'
```

## Implementation Overview
#### Architecture & Approach
This microservice fetches game transactions from a mocked API, aggregates them per user, and persists the results in MongoDB. It avoids storing individual transactions and instead focuses on maintaining summary statistics such as:
earned
spent
payout
paidOut
balance

A cron-based background job runs every 12 seconds and fetches up to 5 pages (1000 transactions each) using an afterId pointer. This ID is stored in MongoDB (sync_state collection) to ensure resumption in case of service restarts.

#### Assumptions
- Transactions are returned in chronological order and contain a unique, sortable id.
- The external transaction API supports pagination using afterId (or can be adapted to).
- No transaction retry or deduplication logic needed â€” all transactions are unique and processed exactly once.
- Transactions older than afterId are guaranteed to have already been processed.

## Scaling
### Running Multiple Aggregators in Parallel
If you want to horizontally scale the aggregator service by running multiple instances in parallel, `afterId`-based pagination must be adjusted to avoid duplication or data loss.

The current implementation assumes a single global `afterId` pointer. If multiple instances use this shared cursor, it can cause:
- Duplicate transaction processing
- Skipped transactions
- Race conditions updating `afterId`

#### Replace `afterId` with a Distributed Job Queue
Use a job queue like **BullMQ**, **RabbitMQ**, or **Kafka** to split fetch jobs by ID or time. Each worker processes its own batch without relying on a shared pointer.

#### Use a Locking Cursor If Staying with afterId
If sticking to `afterId`, introduce a **lock mechanism**:
- A shared `sync_state` doc tracks the cursor
- Workers acquire the lock, process a page, update `afterId`, release

### Separate out read aggregator logic in a different microservice 
- Heavy reads can slow down aggregation
- Simplify scaling
- Use MongoDB read replicas

<pre> ## ğŸ§© Timestamp-Based Aggregator System Architecture ```text â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Job Generator (Cron) â”‚ â”‚ [Every minute: create â”‚ â”‚ time window jobs] â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â–¼ â–¼ â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Aggregator â”‚ â”‚ Aggregator â”‚ â”‚ Aggregator â”‚ â”‚ Worker 1 â”‚ â”‚ Worker 2 â”‚ â”‚ Worker 3 â”‚ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â–¼ â–¼ â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Deduplication (Redis) â”‚ â”‚ - Skip seen txnIds â”‚ â”‚ - Mark new txnIds as seen â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ MongoDB (Primary) â”‚ â”‚ - Aggregate data â”‚ â”‚ - $inc, $addToSet â”‚ â”‚ - Idempotent ops â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Read API â”‚ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Load Balancer â”‚ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â–¼ â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Replicaâ”‚ â”‚ Replicaâ”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ ``` --- ## âœ… Key Features - **Job Generator**: Schedules time-window-based fetch jobs (e.g., every minute). - **Aggregator Workers**: Fetch transactions in parallel using time slices. - **Redis Deduplication**: Ensures idempotency across workers. - **MongoDB Primary**: Stores aggregates using `$inc`, `$addToSet` safely. - **Read API**: Fetches data via read replicas for performance. - **Horizontal Scaling**: Add more workers for increased ingestion throughput. </pre>

## Testing
- **Unit Tests**: Validate core logic (e.g., aggregation, payout calculation)
- **Integration Tests**: Test MongoDB operations and API response structure
- **E2E Tests**: Verify full flow from API â†’ DB â†’ Aggregated response
- **Load Tests**: Stress test endpoints to simulate high user traffic
- **Static Analysis**: Type checks (TypeScript), code style, linting

#### TDD Approach
1. Write failing test for each unit:
  Aggregator logic (e.g., "earned + spent = balance")
  Pagination from API using afterId
  Payout summarization
  Deduplication
2. Implement logic minimally to pass test
3. Refactor for clarity, performance
4. Repeat


